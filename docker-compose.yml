version: "3.9"
services:
  satcat_ingest:
    build:
      context: ./satcat_ingest
    container_name: nl2sql_satcat_ingest
    environment:
      ST_USER: ${ST_USER}
      ST_PASS: ${ST_PASS}
      PG_DSN: postgresql://app:app_pw@postgres:5432/appdb
      RUN_MODE: oneshot          # ← force one-shot
    depends_on:
      postgres:
        condition: service_healthy
    restart: "no"                 # ← important: do NOT restart
    networks: [nl2sql_net]
    profiles: ["manual"]          # ← optional: keeps it out of normal `up -d`

  postgres:
    image: postgres:16
    container_name: nl2sql_postgres
    environment:
      POSTGRES_USER: app
      POSTGRES_PASSWORD: app_pw
      POSTGRES_DB: appdb
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./db:/docker-entrypoint-initdb.d:ro
    # Needed so other services wait untill Postgres is
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U app -d appdb"]
      interval: 5s
      timeout: 3s
      retries: 30
    networks: [nl2sql_net]



  nl2sql_ollama:
    image: ollama/ollama:latest
    container_name: nl2sql_ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      OLLAMA_NO_MMAP: "1"        # force buffered I/O (no memory-mapping)
      OLLAMA_LOAD_TIMEOUT: "30m" # give the first cold load time to finish
      OLLAMA_KEEP_ALIVE: "24h"   # keep the model warm
      OLLAMA_NUM_PARALLEL: "1"   # one request at a time while debugging
      OLLAMA_HOST: "0.0.0.0:11434"
      NVIDIA_VISIBLE_DEVICES: "all"
      OLLAMA_MODEL: "${OLLAMA_MODEL:-llama3.1:8b-instruct-q4_K_M}"
    volumes:
      - ./ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    networks: [nl2sql_net]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 30

  warmup:
    image: curlimages/curl:8.8.0
    container_name: nl2sql_warmup
    depends_on:
      nl2sql_ollama:
        condition: service_started
    environment:
      OLLAMA_MODEL: "llama3.1:8b-instruct-q4_K_M"
      EMBED_MODEL: "nomic-embed-text"
    networks: [nl2sql_net]
    restart: "no"
    command: >
      sh -lc '
        # wait until daemon answers
        until curl -sf http://nl2sql_ollama:11434/api/tags >/dev/null; do sleep 1; done;
        echo "Warming up ${OLLAMA_MODEL:-llama3.1:8b-instruct-q4_K_M} ...";
        curl -sS -X POST http://nl2sql_ollama:11434/api/chat \
          -H "Content-Type: application/json" \
          -d "{\"model\":\"${OLLAMA_MODEL:-llama3.1:8b-instruct-q4_K_M}\",\"messages\":[{\"role\":\"user\",\"content\":\"warmup\"}],\"stream\":false}";
        curl -sS -X POST http://nl2sql_ollama:11434/api/embeddings \
          -H "Content-Type: application/json" \
          -d "{\"model\":\"${EMBED_MODEL:-nomic-embed-text}\",\"messages\":[{\"role\":\"user\",\"content\":\"warmup\"}],\"stream\":false}";      
      '

  python-llm:
    build:
      context: ./services/python-llm
      dockerfile: Dockerfile
    container_name: nl2sql_python_llm
    environment:
      # select mock for offline testing
      LLM_BACKEND: "ollama"
      LLM_PROVIDER: "ollama"
      OLLAMA_URL: ${OLLAMA_URL:-http://nl2sql_ollama:11434}
      OLLAMA_BASE_URL: "http://nl2sql_ollama:11434"
      OLLAMA_MODEL: "${OLLAMA_MODEL:-llama3.1:8b-instruct-q4_K_M}"
      OPENAI_BASE_URL: ${OPENAI_BASE_URL:-https://api.openai.com/v1/chat/completions}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      OPENAI_MODEL: "gpt-4o-mini"
      EMBED_MODEL: "nomic-embed-text"
      APP_PG_DSN: "dbname=appdb user=app password=app_pw host=nl2sql_postgres port=5432"
      RAG_PG_DSN: "dbname=ragdb user=rag password=ragpw host=rag-pg port=5432"
    depends_on:
      nl2sql_ollama:
        condition: service_healthy
      warmup:
        condition: service_completed_successfully
    ports:
      - "8000:8000"
    networks: [nl2sql_net]
    develop:
      watch:
        - path: ./services/python-llm
          action: sync
          target: /app/
        - path: ./services/python-llm/requirements.txt
          action: rebuild

  backend:
    build:
      context: ./backend-java
      dockerfile: Dockerfile
    container_name: nl2sql_backend
    environment:
      SPRING_PROFILES_ACTIVE: docker
      DB_URL: jdbc:postgresql://postgres:5432/appdb
      DB_USERNAME: app
      DB_PASSWORD: app_pw
      PY_SERVICE_BASEURL: http://python-llm:8000
    depends_on:
      - postgres
      - python-llm
    ports:
      - "8080:8080"
    networks: [nl2sql_net]

  rag-pg:
    image: pgvector/pgvector:pg16   # includes pgvector for Postgres 16
    container_name: rag-pg
    environment:
      POSTGRES_USER: rag
      POSTGRES_PASSWORD: ragpw
      POSTGRES_DB: postgres   # we'll create ragdb ourselves for deterministic collation
    volumes:
      - ragpgdata:/var/lib/postgresql/data
    networks: [nl2sql_net]

  tle_ingest:
    build: ./tle_ingest
    environment:
      ST_USERNAME: ${ST_USER}
      ST_PASSWORD: ${ST_PASS}
      PGHOST: postgres
      PGPORT: 5432
      PGDATABASE: appdb
      PGUSER: app
      PGPASSWORD: ${DB_PASSWORD}
      MODE: once                # "once" for manual runs
      BATCH_SIZE: 100
      INTER_BATCH_SLEEP: 12.0
    depends_on:
      - postgres
    networks:
      - nl2sql_net
    restart: "no"
    profiles: ["manual"]
      
  rag-ingest:
    image: python:3.11-slim
    working_dir: /app
    volumes:
      - ./rag_ingest:/app                # put ingest_schema.py under ./rag
    environment:
      EMBED_MODEL: "nomic-embed-text"
      APP_PG_DSN: "dbname=appdb user=app password=app_pw host=nl2sql_postgres port=5432"
      RAG_PG_DSN: "dbname=ragdb user=rag password=ragpw host=rag-pg port=5432"
      OLLAMA_URL: "http://nl2sql_ollama:11434"
    command: >
      sh -lc "pip install --no-cache-dir requests psycopg2-binary &&
              python ingest_schema.py"
    depends_on:
      - postgres                  # or your actual postgres service name
      - nl2sql_ollama
    networks:
      - nl2sql_net

volumes:
  pgdata:
  ollama_models:
  ragpgdata:

networks:
  nl2sql_net:
    driver: bridge
